# Deep_learning_models
# Comparative study of Different Deep Learning Models on CIFAR10 Dataset

This repository presents a comparative study of four deep learning architectures - LeNet, ResNet , VGG16 and Transformer - applied to CIFAR10 image classification task.
## Model Implemented

- LeNet : First CNN architectures used for handwritten digit recognition.
- ResNet : Deep residual network with skip connections.
- VGG16 : Deep CNN with a reputation for stacking convolutional layers and small receptive fields.
- Transformer: A neural network architecture based on self-attention mechanisms,originally developed for NLP.

## Evaluation Metrics

- Accuracy
- Precision
- Recall
## Results Summary



| Model | Accuracy(%) | Precision(%) |  Recall(%)       |
| :----------- | :------- | :--------- | :------------------ |
| LeNet | 61.12 | 60.80 | 61.12 |
| ResNet| 81.78 | 81.74 | 81.78 |
| VGG16 | 61.26 | 61.12 | 61.26 |
| Transformer | 62.35 | 62.27 | 62.35 |


## Dataset

- CIFAR10 Dataset
- 60,000 images (32x32 pixels)
- Training set - 50,000
- Test set - 10,000
- Classes - 10 

## Requirements

- Python 3.8+
- TensorFlow/Keras
- Libraries : Numpy,Matplotlib,Scikit-learn
------

Platform - Google Colab
-----
Submitted by - Shivangi
